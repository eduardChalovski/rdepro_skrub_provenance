{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# AggJoiner on a credit fraud dataset\n",
    "\n",
    "Many problems involve tables whose entities have a one-to-many relationship.\n",
    "To simplify aggregate-then-join operations for machine learning, we can include\n",
    "the |AggJoiner| in our pipeline.\n",
    "\n",
    "\n",
    "In this example, we are tackling a fraudulent loan detection use case.\n",
    "Because fraud is rare, this dataset is extremely imbalanced, with a prevalence of around\n",
    "1.4%.\n",
    "\n",
    "The data consists of two distinct entities: e-commerce \"baskets\", and \"products\".\n",
    "Baskets can be tagged fraudulent (1) or not (0), and are essentially a list of products\n",
    "of variable size. Each basket is linked to at least one products, e.g. basket 1 can have\n",
    "product 1 and 2.\n",
    "\n",
    "<img src=\"file://../../_static/08_example_data.png\" width=\"450 px\">\n",
    "\n",
    "|\n",
    "\n",
    "Our aim is to predict which baskets are fraudulent.\n",
    "\n",
    "The products dataframe can be joined on the baskets dataframe using the ``basket_ID``\n",
    "column.\n",
    "\n",
    "Each product has several attributes:\n",
    "\n",
    "- a category (marked by the column ``\"item\"``),\n",
    "- a model (``\"model\"``),\n",
    "- a brand (``\"make\"``),\n",
    "- a merchant code (``\"goods_code\"``),\n",
    "- a price per unit (``\"cash_price\"``),\n",
    "- a quantity selected in the basket (``\"Nbr_of_prod_purchas\"``)\n",
    "\n",
    ".. |AggJoiner| replace::\n",
    "     :class:`~skrub.AggJoiner`\n",
    "\n",
    ".. |Joiner| replace::\n",
    "     :class:`~skrub.Joiner`\n",
    "\n",
    ".. |DropCols| replace::\n",
    "     :class:`~skrub.DropCols`\n",
    "\n",
    ".. |TableVectorizer| replace::\n",
    "     :class:`~skrub.TableVectorizer`\n",
    "\n",
    ".. |TableReport| replace::\n",
    "     :class:`~skrub.TableReport`\n",
    "\n",
    ".. |MinHashEncoder| replace::\n",
    "     :class:`~skrub.MinHashEncoder`\n",
    "\n",
    ".. |TargetEncoder| replace::\n",
    "     :class:`~sklearn.preprocessing.TargetEncoder`\n",
    "\n",
    ".. |make_pipeline| replace::\n",
    "     :func:`~sklearn.pipeline.make_pipeline`\n",
    "\n",
    ".. |Pipeline| replace::\n",
    "     :class:`~sklearn.pipeline.Pipeline`\n",
    "\n",
    ".. |HGBC| replace::\n",
    "     :class:`~sklearn.ensemble.HistGradientBoostingClassifier`\n",
    "\n",
    ".. |OrdinalEncoder| replace::\n",
    "     :class:`~sklearn.preprocessing.OrdinalEncoder`\n",
    "\n",
    ".. |TunedThresholdClassifierCV| replace::\n",
    "     :class:`~sklearn.model_selection.TunedThresholdClassifierCV`\n",
    "\n",
    ".. |CalibrationDisplay| replace::\n",
    "     :class:`~sklearn.calibration.CalibrationDisplay`\n",
    "\n",
    ".. |pandas.melt| replace::\n",
    "     :func:`~pandas.melt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from skrub import TableReport\n",
    "from skrub.datasets import fetch_credit_fraud\n",
    "\n",
    "bunch = fetch_credit_fraud()\n",
    "products, baskets = bunch.products, bunch.baskets\n",
    "TableReport(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "TableReport(baskets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive aggregation\n",
    "\n",
    "Let's explore a naive solution first.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Click `here<agg-joiner-anchor>` to skip this section and see the AggJoiner\n",
    "   in action!</p></div>\n",
    "\n",
    "\n",
    "The first idea that comes to mind to merge these two tables is to aggregate the\n",
    "products attributes into lists, using their basket IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "products_grouped = products.groupby(\"basket_ID\").agg(list)\n",
    "TableReport(products_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can expand all lists into columns, as if we were \"flattening\" the dataframe.\n",
    "We end up with a products dataframe ready to be joined on the baskets dataframe, using\n",
    "``\"basket_ID\"`` as the join key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "products_flatten = []\n",
    "for col in products_grouped.columns:\n",
    "    cols = [f\"{col}{idx}\" for idx in range(24)]\n",
    "    products_flatten.append(pd.DataFrame(products_grouped[col].to_list(), columns=cols))\n",
    "products_flatten = pd.concat(products_flatten, axis=1)\n",
    "products_flatten.insert(0, \"basket_ID\", products_grouped.index)\n",
    "TableReport(products_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the \"Stats\" section of the |TableReport| above. Does anything strike you?\n",
    "\n",
    "Not only did we create 144 columns, but most of these columns are filled with NaN,\n",
    "which is very inefficient for learning!\n",
    "\n",
    "This is because each basket contains a variable number of products, up to 24, and we\n",
    "created one column for each product attribute, for each position (up to 24) in\n",
    "the dataframe.\n",
    "\n",
    "Moreover, if we wanted to replace text columns with encodings, we would create\n",
    "$d \\times 24 \\times 2$ columns (encoding of dimensionality $d$, for\n",
    "24 products, for the ``\"item\"`` and ``\"make\"`` columns), which would explode the\n",
    "memory usage.\n",
    "\n",
    "\n",
    "## AggJoiner\n",
    "Let's now see how the |AggJoiner| can help us solve this. We begin with splitting our\n",
    "basket dataset in a training and testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = baskets[[\"ID\"]], baskets[\"fraud_flag\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before aggregating our product dataframe, we need to vectorize our categorical\n",
    "columns. To do so, we use:\n",
    "\n",
    "- |MinHashEncoder| on \"item\" and \"model\" columns, because they both expose typos\n",
    "  and text similarities.\n",
    "- |OrdinalEncoder| on \"make\" and \"goods_code\" columns, because they consist in\n",
    "  orthogonal categories.\n",
    "\n",
    "We bring this logic into a |TableVectorizer| to vectorize these columns in a\n",
    "single step.\n",
    "See [this example](https://skrub-data.org/stable/auto_examples/01_encodings.html#specializing-the-tablevectorizer-for-histgradientboosting)\n",
    "for more details about these encoding choices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from skrub import MinHashEncoder, TableVectorizer\n",
    "\n",
    "vectorizer = TableVectorizer(\n",
    "    high_cardinality=MinHashEncoder(),  # encode [\"item\", \"model\"]\n",
    "    specific_transformers=[\n",
    "        (OrdinalEncoder(), [\"make\", \"goods_code\"]),\n",
    "    ],\n",
    ")\n",
    "products_transformed = vectorizer.fit_transform(products)\n",
    "TableReport(products_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is now to aggregate this vectorized product dataframe by\n",
    "``\"basket_ID\"``, then to merge it on the baskets dataframe, still on\n",
    "the ``\"basket_ID\"``.\n",
    "\n",
    "<img src=\"file://../../_static/08_example_aggjoiner.png\" width=\"900\">\n",
    "\n",
    "|\n",
    "\n",
    "|AggJoiner| can help us achieve exactly this. We need to pass the product dataframe as\n",
    "an auxiliary table argument to |AggJoiner| in ``__init__``. The ``aux_key`` argument\n",
    "represent both the columns used to groupby on, and the columns used to join on.\n",
    "\n",
    "The basket dataframe is our main table, and we indicate the columns to join on with\n",
    "``main_key``. Note that we pass the main table during ``fit``, and we discuss the\n",
    "limitations of this design in the conclusion at the bottom of this notebook.\n",
    "\n",
    "The minimum (\"min\") is the most appropriate operation to aggregate encodings from\n",
    "|MinHashEncoder|, for reasons that are out of the scope of this notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from skrub import AggJoiner\n",
    "from skrub import selectors as s\n",
    "\n",
    "# Skrub selectors allow us to select columns using regexes, which reduces\n",
    "# the boilerplate.\n",
    "minhash_cols_query = s.glob(\"item*\") | s.glob(\"model*\")\n",
    "minhash_cols = s.select(products_transformed, minhash_cols_query).columns\n",
    "\n",
    "agg_joiner = AggJoiner(\n",
    "    aux_table=products_transformed,\n",
    "    aux_key=\"basket_ID\",\n",
    "    main_key=\"ID\",\n",
    "    cols=minhash_cols,\n",
    "    operations=[\"min\"],\n",
    ")\n",
    "baskets_products = agg_joiner.fit_transform(baskets)\n",
    "TableReport(baskets_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how to use the |AggJoiner|, we can now assemble our pipeline by\n",
    "chaining two |AggJoiner| together:\n",
    "\n",
    "- the first one to deal with the |MinHashEncoder| vectors as we just saw\n",
    "- the second one to deal with the all the other columns\n",
    "\n",
    "For the second |AggJoiner|, we use the mean, standard deviation, minimum and maximum\n",
    "operations to extract a representative summary of each distribution.\n",
    "\n",
    "|DropCols| is another skrub transformer which removes the \"ID\" column, which doesn't\n",
    "bring any information after the joining operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, randint\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from skrub import DropCols\n",
    "\n",
    "model = make_pipeline(\n",
    "    AggJoiner(\n",
    "        aux_table=products_transformed,\n",
    "        aux_key=\"basket_ID\",\n",
    "        main_key=\"ID\",\n",
    "        cols=minhash_cols,\n",
    "        operations=[\"min\"],\n",
    "    ),\n",
    "    AggJoiner(\n",
    "        aux_table=products_transformed,\n",
    "        aux_key=\"basket_ID\",\n",
    "        main_key=\"ID\",\n",
    "        cols=[\"make\", \"goods_code\", \"cash_price\", \"Nbr_of_prod_purchas\"],\n",
    "        operations=[\"sum\", \"mean\", \"std\", \"min\", \"max\"],\n",
    "    ),\n",
    "    DropCols([\"ID\"]),\n",
    "    HistGradientBoostingClassifier(),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tune the hyper-parameters of the |HGBC| model using ``RandomizedSearchCV``.\n",
    "By default, the |HGBC| applies early stopping when there are at least 10_000\n",
    "samples so we don't need to explicitly tune the number of trees (``max_iter``).\n",
    "Therefore we set this at a very high level of 1_000. We increase\n",
    "``n_iter_no_change`` to make sure early stopping does not kick in too early.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = dict(\n",
    "    histgradientboostingclassifier__learning_rate=loguniform(1e-2, 5e-1),\n",
    "    histgradientboostingclassifier__min_samples_leaf=randint(2, 64),\n",
    "    histgradientboostingclassifier__max_leaf_nodes=[None, 10, 30, 60, 90],\n",
    "    histgradientboostingclassifier__n_iter_no_change=[50],\n",
    "    histgradientboostingclassifier__max_iter=[1000],\n",
    ")\n",
    "\n",
    "tic = time()\n",
    "search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions,\n",
    "    scoring=\"neg_log_loss\",\n",
    "    refit=False,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    ").fit(X_train, y_train)\n",
    "print(f\"This operation took {time() - tic:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyper parameters are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To benchmark our performance, we plot the log loss of our model on the test set\n",
    "against the log loss of a dummy model that always output the observed probability of\n",
    "the two classes.\n",
    "\n",
    "As this dataset is extremely imbalanced, this dummy model should be a good baseline.\n",
    "\n",
    "The vertical bar represents one standard deviation around the mean of the cross\n",
    "validation log-loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "results = search.cv_results_\n",
    "best_idx = search.best_index_\n",
    "log_loss_model_mean = -results[\"mean_test_score\"][best_idx]\n",
    "log_loss_model_std = results[\"std_test_score\"][best_idx]\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"prior\").fit(X_train, y_train)\n",
    "y_proba_dummy = dummy.predict_proba(X_test)\n",
    "log_loss_dummy = log_loss(y_true=y_test, y_pred=y_proba_dummy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(\n",
    "    height=[log_loss_model_mean, log_loss_dummy],\n",
    "    x=[\"AggJoiner model\", \"Dummy\"],\n",
    "    color=[\"C0\", \"C4\"],\n",
    ")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, padding=4)\n",
    "\n",
    "ax.vlines(\n",
    "    x=\"AggJoiner model\",\n",
    "    ymin=log_loss_model_mean - log_loss_model_std,\n",
    "    ymax=log_loss_model_mean + log_loss_model_std,\n",
    "    linestyle=\"-\",\n",
    "    linewidth=1,\n",
    "    color=\"k\",\n",
    ")\n",
    "sns.despine()\n",
    "ax.set_title(\"Log loss (lower is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "With |AggJoiner|, you can bring the aggregation and joining operations within a\n",
    "sklearn pipeline, and train models more efficiently.\n",
    "\n",
    "One known limitation of both the |AggJoiner| and |Joiner| is that the auxiliary data\n",
    "to join is passed during the ``__init__`` method instead of the ``fit`` method, and\n",
    "is therefore fixed once the model has been trained.\n",
    "This limitation causes two main issues:\n",
    "\n",
    "1. **Bigger model serialization:** Since the dataset has to be pickled along with\n",
    "the model, it can result in a massive file size on disk.\n",
    "\n",
    "2. **Inflexibility with new, unseen data in a production environment:** To use new\n",
    "auxiliary data, you would need to replace the auxiliary table in the |AggJoiner| that\n",
    "was used during ``fit`` with the updated data, which is a rather hacky approach.\n",
    "\n",
    "These limitations will be addressed later in skrub.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
